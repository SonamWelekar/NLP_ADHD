{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path for importing required functions for data processing\n",
    "\n",
    "sys.path.append(\"/home/jupyter/sonam/adhd_nlp/final_notebook_folder/data_processing\")\n",
    "sys.path.append(\"/home/jupyter/sonam/adhd_nlp/final_notebook_folder/data_processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions required for data processing\n",
    "\n",
    "import final_process_text\n",
    "import final_transform_textfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using simpletransformer ai library\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_of_interest = \"BT_yn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalTextData = final_transform_textfiles.extractOriginalText(\"/home/jupyter/data/cohort_2to6/Text files/combined_text\")\n",
    "\n",
    "annotatedXMIs = final_transform_textfiles.extractXMIAnnotation(\"/home/jupyter/data/cohort_2to6/XMI files/combined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotatedXMIs['anon_id'].value_counts(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting last row from 'originalTextData' and 'annotatedXMIS' dataframe as the last row is just checpoint roe mentioned above\n",
    "# for  ANNON_ID 10040 and hence deleting it would result in final cohort size of 432 as needed for new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping last row in text and XMI pandas dataframe.\n",
    "#originalTextData.drop(originalTextData.tail(-1).index,inplace=True)\n",
    "# annotatedXMIs.drop(annotatedXMIs.tail(-1).index,inplace=True)\n",
    "\n",
    "originalTextData = originalTextData[:-1]\n",
    "annotatedXMIs = annotatedXMIs[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating single label_of_interest \"BT_yn\" column using other columns\n",
    "\n",
    "annotatedXMIs['BT_yn'] = np.where((annotatedXMIs['Counsel_Parent_BT'] == 1) | (annotatedXMIs['Counsel_Handout_BT'] == 1) | (annotatedXMIs['Refer_Parent_BT'] == 1) | (annotatedXMIs['Refer_School_BT'] == 1), 1, 0)\n",
    "annotatedXMIs['BT_yn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging data from both files \n",
    "data = originalTextData.merge(annotatedXMIs, on = \"file\", how = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function sectionize() for processing notes text data\n",
    "\n",
    "data['extractText'] = data['note_des'].apply(lambda x: final_process_text.sectionize(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function clean_text() for processing notes text data\n",
    "\n",
    "data['extractText'] = data['extractText'].apply(lambda x: final_process_text.clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[:, ['extractText',label_of_interest]]\\\n",
    "       .rename(columns = {'extractText':'text',\n",
    "                          label_of_interest: 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, 'text']\n",
    "y = data.loc[:, 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 117, stratify = y)\n",
    "X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_train, y_train, test_size = 0.3, random_state = 117, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_train = pd.concat([X_val_train, y_val_train], axis = 1)\n",
    "val_test = pd.concat([X_val_test, y_val_test], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)\n",
    "\n",
    "#test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting different parameters for training the transformer model\n",
    "\n",
    "model_args = ClassificationArgs()\n",
    "batch_size = 8\n",
    "steps_per_epoch = np.ceil(val_train.shape[0]/batch_size)\n",
    "model_args.num_train_epochs = 30\n",
    "\n",
    "model_args.eval_batch_size = batch_size\n",
    "model_args.train_batch_size = batch_size\n",
    "model_args.manual_seed = 117\n",
    "\n",
    "model_args.evaluate_during_training_steps = steps_per_epoch\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "model_args.max_seq_length = 512\n",
    "model_args.learning_rate = 0.00008\t\n",
    "\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_steps = -1\n",
    "model_args.logging_steps = steps_per_epoch\n",
    "\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.manual_seed = 117\n",
    "model_args.evaluate_during_training = True\n",
    "\n",
    "model_args.best_model_dir = 'final_biobertbest_model_dir_new_cohort'\n",
    "model_args.output_dir = 'final_biobert_output_dir_new_cohort'\n",
    "model_args.tensorboard_dir = 'final_biobert_tensorboard_runs_new_cohort'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel(\"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args, use_cuda = True, num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "start = time.time()\n",
    "model.train_model(val_train, f1 = f1_score, eval_df = val_test)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluate on Validation set (89 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result, model_outputs, wrong_predictions = model.eval_model(val_test, f1 = f1_score, \n",
    "                                                            recall = sklearn.metrics.recall_score,\n",
    "                                                            precision = sklearn.metrics.precision_score,\n",
    "                                                            auc = sklearn.metrics.roc_auc_score,\n",
    "                                                            accuracy = sklearn.metrics.accuracy_score)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving misclassified samples for validation set in file\n",
    "with open('/home/jupyter/sonam/final_result_files/final_val_misclassification_new_cohort.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    list=[\"index\", \"truth\", \"error\",\"tokenLength\", \"text\"]\n",
    "    myfile.writelines(str(list));\n",
    "    myfile.writelines(\"\\n\");\n",
    "    for x in range(len(wrong_predictions)): \n",
    "        list=[];\n",
    "        list.append(x+1);\n",
    "        truth=wrong_predictions[x].label;\n",
    "        list.append(truth);\n",
    "        if truth==1: list.append(\"fn\")\n",
    "        elif truth==0: list.append(\"fp\")\n",
    "        list.append(len(model.tokenizer(wrong_predictions[x].text_a)['input_ids']));\n",
    "        list.append(wrong_predictions[x].text_a);\n",
    "        myfile.writelines(str(list));\n",
    "        myfile.writelines(\"\\n\");\n",
    "myfile.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluate on Test set (127 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(test, f1 = f1_score,  \n",
    "                                                            recall = sklearn.metrics.recall_score,\n",
    "                                                            precision = sklearn.metrics.precision_score,\n",
    "                                                            auc = sklearn.metrics.roc_auc_score,\n",
    "                                                           accuracy = sklearn.metrics.accuracy_score)\n",
    "\n",
    "\n",
    "predictions, probabilities = model.predict(test['text'].tolist())\n",
    "test['predictions'] = predictions\n",
    "test['probabilities'] = [x[1] for x in np.array([softmax(element) for element in probabilities])]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting precision, recall, thresholds metrics for test set and calculating threshold f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(test['label'], test['probabilities'])\n",
    "precision = precision[:-1]\n",
    "recall = recall[:-1]\n",
    "thresholdDF = pd.DataFrame(data = {'precision': precision, 'recall': recall, 'thresholds':thresholds})\n",
    "thresholdDF['f1_score'] = 2*(thresholdDF['precision']*thresholdDF['recall'])/(thresholdDF['precision'] + thresholdDF['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(thresholdDF['f1_score'])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
