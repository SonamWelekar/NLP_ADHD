{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path for importing required functions for data processing\n",
    "\n",
    "sys.path.append(\"/home/jupyter/sonam/adhd_nlp/final_notebook_folder/data_processing\")\n",
    "sys.path.append(\"/home/jupyter/sonam/adhd_nlp/final_notebook_folder/data_processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions required for data processing\n",
    "\n",
    "import final_process_text\n",
    "import final_transform_textfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using simpletransformer ai library\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_of_interest = \"BT_yn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalTextData = final_transform_textfiles.extractOriginalText(\"/home/jupyter/data/cohort_2to6/Text files/combined_text\")\n",
    "\n",
    "annotatedXMIs = final_transform_textfiles.extractXMIAnnotation(\"/home/jupyter/data/cohort_2to6/XMI files/combined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10040    2\n",
      "58356    1\n",
      "60206    1\n",
      "32135    1\n",
      "29371    1\n",
      "        ..\n",
      "11743    1\n",
      "14870    1\n",
      "69011    1\n",
      "10235    1\n",
      "71104    1\n",
      "Name: anon_id, Length: 423, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(annotatedXMIs['anon_id'].value_counts(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting last row from 'originalTextData' and 'annotatedXMIS' dataframe as the last row is just checpoint row mentioned above\n",
    "# for  ANNON_ID 10040 and hence deleting it would result in final cohort size of 432 as needed for new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping last row in text and XMI pandas dataframe.\n",
    "#originalTextData.drop(originalTextData.tail(-1).index,inplace=True)\n",
    "# annotatedXMIs.drop(annotatedXMIs.tail(-1).index,inplace=True)\n",
    "\n",
    "originalTextData = originalTextData[:-1]\n",
    "annotatedXMIs = annotatedXMIs[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    303\n",
       "1    120\n",
       "Name: BT_yn, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating single label_of_interest \"BT_yn\" column using other columns\n",
    "\n",
    "annotatedXMIs['BT_yn'] = np.where((annotatedXMIs['Counsel_Parent_BT'] == 1) | (annotatedXMIs['Counsel_Handout_BT'] == 1) | (annotatedXMIs['Refer_Parent_BT'] == 1) | (annotatedXMIs['Refer_School_BT'] == 1), 1, 0)\n",
    "annotatedXMIs['BT_yn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging data from both files \n",
    "data = originalTextData.merge(annotatedXMIs, on = \"file\", how = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function sectionize() for processing notes text data\n",
    "\n",
    "data['extractText'] = data['note_des'].apply(lambda x: final_process_text.sectionize(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function clean_text() for processing notes text data\n",
    "\n",
    "data['extractText'] = data['extractText'].apply(lambda x: final_process_text.clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[:, ['extractText',label_of_interest]]\\\n",
    "       .rename(columns = {'extractText':'text',\n",
    "                          label_of_interest: 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, 'text']\n",
    "y = data.loc[:, 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 117, stratify = y)\n",
    "X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_train, y_train, test_size = 0.3, random_state = 117, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_train = pd.concat([X_val_train, y_val_train], axis = 1)\n",
    "val_test = pd.concat([X_val_test, y_val_test], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)\n",
    "\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading saved model and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved weights from the training  for defining the model\n",
    "model = ClassificationModel(\"bert\", \"./final_biobert_output_dir_new_cohort\",use_cuda = True, num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluate on Validation set (89 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a67a22f461643a2bd19ae77f90f60ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=89.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f7774c5aa847cf9e00c0a8d4797c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=12.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.52852868908695,\n",
       " 'tp': 14,\n",
       " 'tn': 59,\n",
       " 'fp': 5,\n",
       " 'fn': 11,\n",
       " 'auroc': 0.76,\n",
       " 'auprc': 0.7235928012436937,\n",
       " 'f1': 0.6363636363636364,\n",
       " 'recall': 0.56,\n",
       " 'precision': 0.7368421052631579,\n",
       " 'auc': 0.7409375,\n",
       " 'accuracy': 0.8202247191011236,\n",
       " 'eval_loss': 1.3299520648154914}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result, model_outputs, wrong_predictions = model.eval_model(val_test, f1 = f1_score, \n",
    "                                                            recall = sklearn.metrics.recall_score,\n",
    "                                                            precision = sklearn.metrics.precision_score,\n",
    "                                                            auc = sklearn.metrics.roc_auc_score,\n",
    "                                                            accuracy = sklearn.metrics.accuracy_score)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a234179e3e940fb927069e2fcb5e938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=89.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5764154af2444842b8d549738be84e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions, probabilities = model.predict(val_test['text'].tolist())\n",
    "val_test['predictions'] = predictions\n",
    "val_test['probabilities'] = [x[1] for x in np.array([softmax(element) for element in probabilities])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to create the precion-recall-f1 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_metrics(true_label , pred_prob):\n",
    "#     true_label= val_test['label']\n",
    "#     pred_prob = val_test['probabilities']\n",
    "\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(true_label, pred_prob)\n",
    "    precision = precision[:-1]\n",
    "    recall = recall[:-1]\n",
    "    f1 = 2*(precision*recall)/(precision+recall)\n",
    "    results_DF = pd.DataFrame(data = {'precision': precision, \n",
    "                                       'recall': recall,\n",
    "                                       'f1' : f1,\n",
    "                                      'thresholds':thresholds})\n",
    "    print(results_DF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to create a confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_thr(threshold_final, true_label, pred_prob):\n",
    "    \n",
    "    pred_label = (pred_prob >= threshold_final)\n",
    "\n",
    "    pred_label = pred_label.values.astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(true_label, pred_label, normalize='true').ravel()\n",
    "    print(sklearn.metrics.classification_report(true_label, pred_label))\n",
    "\n",
    "    print(\"tn:\",tn)\n",
    "    print(\"tp:\",tp)\n",
    "    print(\"fn:\",fn)\n",
    "    print(\"fp:\",fp)\n",
    "    \n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe with all the metrics and threshold to set a threshold value which gives maximum precision with decent recall and f1 score.\n",
      "    precision  recall        f1  thresholds\n",
      "0    0.280899    1.00  0.438596    0.000084\n",
      "1    0.272727    0.96  0.424779    0.000084\n",
      "2    0.289157    0.96  0.444444    0.000084\n",
      "3    0.291139    0.92  0.442308    0.000085\n",
      "4    0.319444    0.92  0.474227    0.000085\n",
      "5    0.313433    0.84  0.456522    0.000085\n",
      "6    0.318182    0.84  0.461538    0.000086\n",
      "7    0.328125    0.84  0.471910    0.000086\n",
      "8    0.333333    0.84  0.477273    0.000086\n",
      "9    0.344262    0.84  0.488372    0.000087\n",
      "10   0.333333    0.80  0.470588    0.000087\n",
      "11   0.338983    0.80  0.476190    0.000088\n",
      "12   0.350877    0.80  0.487805    0.000088\n",
      "13   0.333333    0.72  0.455696    0.000089\n",
      "14   0.346154    0.72  0.467532    0.000089\n",
      "15   0.352941    0.72  0.473684    0.000089\n",
      "16   0.382979    0.72  0.500000    0.000090\n",
      "17   0.409091    0.72  0.521739    0.000090\n",
      "18   0.428571    0.72  0.537313    0.000091\n",
      "19   0.439024    0.72  0.545455    0.000092\n",
      "20   0.450000    0.72  0.553846    0.000094\n",
      "21   0.447368    0.68  0.539683    0.000094\n",
      "22   0.459459    0.68  0.548387    0.000098\n",
      "23   0.472222    0.68  0.557377    0.000099\n",
      "24   0.485714    0.68  0.566667    0.000100\n",
      "25   0.500000    0.68  0.576271    0.000101\n",
      "26   0.515152    0.68  0.586207    0.000102\n",
      "27   0.531250    0.68  0.596491    0.000102\n",
      "28   0.548387    0.68  0.607143    0.000102\n",
      "29   0.533333    0.64  0.581818    0.000106\n",
      "30   0.551724    0.64  0.592593    0.000107\n",
      "31   0.571429    0.64  0.603774    0.000122\n",
      "32   0.592593    0.64  0.615385    0.000123\n",
      "33   0.615385    0.64  0.627451    0.000126\n",
      "34   0.640000    0.64  0.640000    0.000127\n",
      "35   0.666667    0.64  0.653061    0.000176\n",
      "36   0.695652    0.64  0.666667    0.000243\n",
      "37   0.727273    0.64  0.680851    0.000286\n",
      "38   0.761905    0.64  0.695652    0.001842\n",
      "39   0.750000    0.60  0.666667    0.099689\n",
      "40   0.736842    0.56  0.636364    0.991456\n",
      "41   0.777778    0.56  0.651163    0.992865\n",
      "42   0.823529    0.56  0.666667    0.999001\n",
      "43   0.875000    0.56  0.682927    0.999487\n",
      "44   0.866667    0.52  0.650000    0.999672\n",
      "45   0.857143    0.48  0.615385    0.999849\n",
      "46   0.846154    0.44  0.578947    0.999861\n",
      "47   0.916667    0.44  0.594595    0.999872\n",
      "48   0.909091    0.40  0.555556    0.999907\n",
      "49   0.900000    0.36  0.514286    0.999930\n",
      "50   0.888889    0.32  0.470588    0.999931\n",
      "51   1.000000    0.32  0.484848    0.999933\n",
      "52   1.000000    0.28  0.437500    0.999934\n",
      "53   1.000000    0.16  0.275862    0.999934\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataframe with all the metrics and threshold to set a threshold value which gives maximum precision with decent recall and f1 score.\")\n",
    "\n",
    "precision_recall_metrics(val_test['label'], val_test['probabilities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#selected row 38 with threshold of 0.001842 for further calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Creating confusion matrix for Validation set(89 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label_val= val_test['label']\n",
    "pred_prob_val = val_test['probabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89        64\n",
      "           1       0.76      0.64      0.70        25\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.81      0.78      0.79        89\n",
      "weighted avg       0.84      0.84      0.84        89\n",
      "\n",
      "tn: 0.921875\n",
      "tp: 0.64\n",
      "fn: 0.36\n",
      "fp: 0.078125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "confusion_matrix_thr(0.001842,true_label_val,pred_prob_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Creating confusion matrix for Test set(127 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff37e36b24a4e778ee4926729c70b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=127.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c0fde90043462995484d5457a65ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions, test_probabilities = model.predict(test['text'].tolist())\n",
    "\n",
    "test['predictions'] = test_predictions\n",
    "test['probabilities'] = [x[1] for x in np.array([softmax(element) for element in test_probabilities])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_label= test['label']\n",
    "test_pred_prob = test['probabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92        91\n",
      "           1       0.84      0.72      0.78        36\n",
      "\n",
      "    accuracy                           0.88       127\n",
      "   macro avg       0.87      0.83      0.85       127\n",
      "weighted avg       0.88      0.88      0.88       127\n",
      "\n",
      "tn: 0.945054945054945\n",
      "tp: 0.7222222222222222\n",
      "fn: 0.2777777777777778\n",
      "fp: 0.054945054945054944\n"
     ]
    }
   ],
   "source": [
    "# Saving predictions obtained for test set using threshold value in arr\n",
    "arr =confusion_matrix_thr(0.001842,test_true_label,test_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating predictions for test set with new predictions obtained according to threshold value\n",
    "\n",
    "test['predictions']= arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to get the dataframe for missclassified samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WP(test):\n",
    "    \n",
    "    index= test.index\n",
    "    condition = ((test['label'] != test['predictions']))\n",
    "    missclassified_indices = index[condition]\n",
    "\n",
    "    missclassified_indices_list = missclassified_indices.tolist() \n",
    "    miss_df = test.loc[test.index.isin(missclassified_indices_list)]\n",
    "    return miss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_miss_df = get_WP(test)\n",
    "\n",
    "wrong_predictions = test_miss_df.to_records(index=True)\n",
    "len(wrong_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#saving the misclassified notes in file for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/jupyter/sonam/final_result_files/final_threshold_test_misclassification_new_cohort.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    list=[\"index\",\"tokenLength\",\"text\", \"truth\", \"error\"]\n",
    "    myfile.writelines(str(list));\n",
    "    myfile.writelines(\"\\n\");\n",
    "    for x in range(len(wrong_predictions)): \n",
    "        list=[];\n",
    "        for y in range(3): \n",
    "            if(y==0):\n",
    "                list.append(wrong_predictions[x][y])\n",
    "            elif (y ==2):\n",
    "                truth=wrong_predictions[x][y];\n",
    "                list.append(truth);\n",
    "                if truth==1: list.append(\"fn\")\n",
    "                elif truth==0: list.append(\"fp\")\n",
    "            elif(y==1):\n",
    "                list.append(len(model.tokenizer(wrong_predictions[x][y])['input_ids']));\n",
    "                list.append(wrong_predictions[x][y]);\n",
    "        myfile.writelines(str(list));\n",
    "        myfile.writelines(\"\\n\");\n",
    "myfile.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Checking saved model on Train Set(207 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97faa27f2c08416fbf565c4f3b4b8789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=207.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe69a455f14e4cb7a87d3d23cab6e360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=26.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mcc': 1.0,\n",
       " 'tp': 59,\n",
       " 'tn': 148,\n",
       " 'fp': 0,\n",
       " 'fn': 0,\n",
       " 'auroc': 1.0,\n",
       " 'auprc': 1.0,\n",
       " 'f1': 1.0,\n",
       " 'recall': 1.0,\n",
       " 'precision': 1.0,\n",
       " 'auc': 1.0,\n",
       " 'accuracy': 1.0,\n",
       " 'eval_loss': 8.889194885761334e-05}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(val_train, f1 = f1_score,  \n",
    "                                                            recall = sklearn.metrics.recall_score,\n",
    "                                                            precision = sklearn.metrics.precision_score,\n",
    "                                                            auc = sklearn.metrics.roc_auc_score,\n",
    "                                                           accuracy = sklearn.metrics.accuracy_score)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting precision, recall, and thresholds for Test SEt (127 samples )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(test['label'], test['predictions'])\n",
    "precision = precision[:-1]\n",
    "recall = recall[:-1]\n",
    "thresholdDF = pd.DataFrame(data = {'precision': precision, 'recall': recall, 'thresholds':thresholds})\n",
    "thresholdDF['f1_score'] = 2*(thresholdDF['precision']*thresholdDF['recall'])/(thresholdDF['precision'] + thresholdDF['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7761194029850746"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(thresholdDF['f1_score'])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
