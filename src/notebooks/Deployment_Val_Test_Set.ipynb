{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path for importing required functions for data processing\n",
    "\n",
    "sys.path.append(\"/home/jupyter/sonam/adhd_nlp/final_notebook_folder/data_processing\")\n",
    "sys.path.append(\"/home/jupyter/sonam/adhd_nlp/final_notebook_folder/data_processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions required for data processing\n",
    "\n",
    "import final_process_text\n",
    "import final_transform_textfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using simpletransformer ai library\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_of_interest = \"BT_yn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalTextData = final_transform_textfiles.extractOriginalText(\"/home/jupyter/data/cohort_2to6/Text files/combined_text\")\n",
    "\n",
    "annotatedXMIs = final_transform_textfiles.extractXMIAnnotation(\"/home/jupyter/data/cohort_2to6/XMI files/combined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotatedXMIs['anon_id'].value_counts(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting last row from 'originalTextData' and 'annotatedXMIS' dataframe as the last row is just checpoint row mentioned above\n",
    "# for  ANNON_ID 10040 and hence deleting it would result in final cohort size of 432 as needed for new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping last row in text and XMI pandas dataframe.\n",
    "#originalTextData.drop(originalTextData.tail(-1).index,inplace=True)\n",
    "# annotatedXMIs.drop(annotatedXMIs.tail(-1).index,inplace=True)\n",
    "\n",
    "originalTextData = originalTextData[:-1]\n",
    "annotatedXMIs = annotatedXMIs[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating single label_of_interest \"BT_yn\" column using other columns\n",
    "\n",
    "annotatedXMIs['BT_yn'] = np.where((annotatedXMIs['Counsel_Parent_BT'] == 1) | (annotatedXMIs['Counsel_Handout_BT'] == 1) | (annotatedXMIs['Refer_Parent_BT'] == 1) | (annotatedXMIs['Refer_School_BT'] == 1), 1, 0)\n",
    "annotatedXMIs['BT_yn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging data from both files \n",
    "data = originalTextData.merge(annotatedXMIs, on = \"file\", how = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function sectionize() for processing notes text data\n",
    "\n",
    "data['extractText'] = data['note_des'].apply(lambda x: final_process_text.sectionize(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function clean_text() for processing notes text data\n",
    "\n",
    "data['extractText'] = data['extractText'].apply(lambda x: final_process_text.clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[:, ['extractText',label_of_interest]]\\\n",
    "       .rename(columns = {'extractText':'text',\n",
    "                          label_of_interest: 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, 'text']\n",
    "y = data.loc[:, 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 117, stratify = y)\n",
    "X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_train, y_train, test_size = 0.3, random_state = 117, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_train = pd.concat([X_val_train, y_val_train], axis = 1)\n",
    "val_test = pd.concat([X_val_test, y_val_test], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)\n",
    "\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading saved model and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved weights from the training  for defining the model\n",
    "model = ClassificationModel(\"bert\", \"./final_biobert_output_dir_new_cohort\",use_cuda = True, num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluate on Validation set (89 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result, model_outputs, wrong_predictions = model.eval_model(val_test, f1 = f1_score, \n",
    "                                                            recall = sklearn.metrics.recall_score,\n",
    "                                                            precision = sklearn.metrics.precision_score,\n",
    "                                                            auc = sklearn.metrics.roc_auc_score,\n",
    "                                                            accuracy = sklearn.metrics.accuracy_score)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, probabilities = model.predict(val_test['text'].tolist())\n",
    "val_test['predictions'] = predictions\n",
    "val_test['probabilities'] = [x[1] for x in np.array([softmax(element) for element in probabilities])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to create the precion-recall-f1 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_metrics(true_label , pred_prob):\n",
    "#     true_label= val_test['label']\n",
    "#     pred_prob = val_test['probabilities']\n",
    "\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(true_label, pred_prob)\n",
    "    precision = precision[:-1]\n",
    "    recall = recall[:-1]\n",
    "    f1 = 2*(precision*recall)/(precision+recall)\n",
    "    results_DF = pd.DataFrame(data = {'precision': precision, \n",
    "                                       'recall': recall,\n",
    "                                       'f1' : f1,\n",
    "                                      'thresholds':thresholds})\n",
    "    print(results_DF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to create a confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_thr(threshold_final, true_label, pred_prob):\n",
    "    \n",
    "    pred_label = (pred_prob >= threshold_final)\n",
    "\n",
    "    pred_label = pred_label.values.astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(true_label, pred_label, normalize='true').ravel()\n",
    "    print(sklearn.metrics.classification_report(true_label, pred_label))\n",
    "\n",
    "    print(\"tn:\",tn)\n",
    "    print(\"tp:\",tp)\n",
    "    print(\"fn:\",fn)\n",
    "    print(\"fp:\",fp)\n",
    "    \n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataframe with all the metrics and threshold to set a threshold value which gives maximum precision with decent recall and f1 score.\")\n",
    "\n",
    "precision_recall_metrics(val_test['label'], val_test['probabilities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#selected row 38 with threshold of 0.001842 for further calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Creating confusion matrix for Validation set(89 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label_val= val_test['label']\n",
    "pred_prob_val = val_test['probabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion_matrix_thr(0.001842,true_label_val,pred_prob_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Creating confusion matrix for Test set(127 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions, test_probabilities = model.predict(test['text'].tolist())\n",
    "\n",
    "test['predictions'] = test_predictions\n",
    "test['probabilities'] = [x[1] for x in np.array([softmax(element) for element in test_probabilities])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_label= test['label']\n",
    "test_pred_prob = test['probabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving predictions obtained for test set using threshold value in arr\n",
    "arr =confusion_matrix_thr(0.001842,test_true_label,test_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating predictions for test set with new predictions obtained according to threshold value\n",
    "\n",
    "test['predictions']= arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to get the dataframe for missclassified samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WP(test):\n",
    "    \n",
    "    index= test.index\n",
    "    condition = ((test['label'] != test['predictions']))\n",
    "    missclassified_indices = index[condition]\n",
    "\n",
    "    missclassified_indices_list = missclassified_indices.tolist() \n",
    "    miss_df = test.loc[test.index.isin(missclassified_indices_list)]\n",
    "    return miss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_miss_df = get_WP(test)\n",
    "\n",
    "wrong_predictions = test_miss_df.to_records(index=True)\n",
    "len(wrong_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#saving the misclassified notes in file for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jupyter/sonam/final_result_files/final_threshold_test_misclassification_new_cohort.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    list=[\"index\",\"tokenLength\",\"text\", \"truth\", \"error\"]\n",
    "    myfile.writelines(str(list));\n",
    "    myfile.writelines(\"\\n\");\n",
    "    for x in range(len(wrong_predictions)): \n",
    "        list=[];\n",
    "        for y in range(3): \n",
    "            if(y==0):\n",
    "                list.append(wrong_predictions[x][y])\n",
    "            elif (y ==2):\n",
    "                truth=wrong_predictions[x][y];\n",
    "                list.append(truth);\n",
    "                if truth==1: list.append(\"fn\")\n",
    "                elif truth==0: list.append(\"fp\")\n",
    "            elif(y==1):\n",
    "                list.append(len(model.tokenizer(wrong_predictions[x][y])['input_ids']));\n",
    "                list.append(wrong_predictions[x][y]);\n",
    "        myfile.writelines(str(list));\n",
    "        myfile.writelines(\"\\n\");\n",
    "myfile.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Checking saved model on Train Set(207 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(val_train, f1 = f1_score,  \n",
    "                                                            recall = sklearn.metrics.recall_score,\n",
    "                                                            precision = sklearn.metrics.precision_score,\n",
    "                                                            auc = sklearn.metrics.roc_auc_score,\n",
    "                                                           accuracy = sklearn.metrics.accuracy_score)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting precision, recall, and thresholds for Test SEt (127 samples )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(test['label'], test['predictions'])\n",
    "precision = precision[:-1]\n",
    "recall = recall[:-1]\n",
    "thresholdDF = pd.DataFrame(data = {'precision': precision, 'recall': recall, 'thresholds':thresholds})\n",
    "thresholdDF['f1_score'] = 2*(thresholdDF['precision']*thresholdDF['recall'])/(thresholdDF['precision'] + thresholdDF['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(thresholdDF['f1_score'])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
